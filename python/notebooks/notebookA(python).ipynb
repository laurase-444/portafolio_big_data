{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164299ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"PythonMnMCount\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05535345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 1 : Leerlo tratando de que Spark infiera el tipo de dato de cada columna, y cachearlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff5cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n",
    "accidentesDF = (spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load('C:/Users/laura.serrano/Desktop/accidents_uk.csv')).cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beebc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "assert(accidentesDF.schema[1].dataType == DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e19c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accident_Index: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Urban_or_Rural_Area: string (nullable = true)\n",
      " |-- X1st_Road_Class: string (nullable = true)\n",
      " |-- Driver_IMD_Decile: integer (nullable = true)\n",
      " |-- Speed_limit: integer (nullable = true)\n",
      " |-- Road_Type: string (nullable = true)\n",
      " |-- Road_Surface_Conditions: string (nullable = true)\n",
      " |-- Weather: string (nullable = true)\n",
      " |-- High_Wind: string (nullable = true)\n",
      " |-- Lights: string (nullable = true)\n",
      " |-- Datetime: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Season: integer (nullable = true)\n",
      " |-- Month_of_Year: integer (nullable = true)\n",
      " |-- Day_of_Month: integer (nullable = true)\n",
      " |-- Day_of_Week: integer (nullable = true)\n",
      " |-- Hour_of_Day: double (nullable = true)\n",
      " |-- Number_of_Vehicles: integer (nullable = true)\n",
      " |-- Age_of_Driver: integer (nullable = true)\n",
      " |-- Age_of_Vehicle: integer (nullable = true)\n",
      " |-- Junction_Detail: string (nullable = true)\n",
      " |-- Junction_Location: string (nullable = true)\n",
      " |-- X1st_Point_of_Impact: string (nullable = true)\n",
      " |-- Driver_Journey_Purpose: string (nullable = true)\n",
      " |-- Engine_CC: integer (nullable = true)\n",
      " |-- Propulsion_Code: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Vehicle_Category: string (nullable = true)\n",
      " |-- Vehicle_Manoeuvre: string (nullable = true)\n",
      " |-- Accident_Severity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accidentesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c572407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 2: Discretizar la variable Age_of_Vehicle utilizando un bucketizer (sin crear un pipeline) en los puntos de corte (0, 2, 5, 10, 15, 20, 35). La discretización debe quedar en una nueva columna de tipo Double llamada Edad_Vehiculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f528fd1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Edad_vehiculo|\n",
      "+-------------+\n",
      "|          2.0|\n",
      "|          1.0|\n",
      "|          2.0|\n",
      "|          1.0|\n",
      "|          3.0|\n",
      "|          1.0|\n",
      "|          3.0|\n",
      "|          2.0|\n",
      "|          0.0|\n",
      "|          1.0|\n",
      "|          2.0|\n",
      "|          1.0|\n",
      "|          1.0|\n",
      "|          3.0|\n",
      "|          2.0|\n",
      "|          3.0|\n",
      "|          2.0|\n",
      "|          1.0|\n",
      "|          2.0|\n",
      "|          2.0|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "# LÍNEAS EVALUABLES, NO RENOMBRAR LAS VARIABLES\n",
    "bucketizer = Bucketizer(splits=[0.0, 2.0, 5.0, 10.0, 15.0, 20.0, 35.0], inputCol=\"Age_of_Vehicle\", outputCol=\"Edad_Vehiculo\")\n",
    "accidentesBucketizedDF = bucketizer.transform(accidentesDF)\n",
    "accidentesBucketizedDF.select(\"Edad_vehiculo\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebb1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\"Edad_Vehiculo\" in accidentesBucketizedDF.columns)\n",
    "assert(accidentesBucketizedDF.schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835c6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 3: Crear un nuevo DF donde la columna \"Age_of_Driver\" ha sido reemplazada por otra de tipo string en la que los valores 1 y 2 son \"Adolescente\", los valores 3 y 4 por \"Joven\", los valores 5 y 6 por \"Adulto\", y los demás valores se dejan sin modificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb51170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accident_Index: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Urban_or_Rural_Area: string (nullable = true)\n",
      " |-- X1st_Road_Class: string (nullable = true)\n",
      " |-- Driver_IMD_Decile: integer (nullable = true)\n",
      " |-- Speed_limit: integer (nullable = true)\n",
      " |-- Road_Type: string (nullable = true)\n",
      " |-- Road_Surface_Conditions: string (nullable = true)\n",
      " |-- Weather: string (nullable = true)\n",
      " |-- High_Wind: string (nullable = true)\n",
      " |-- Lights: string (nullable = true)\n",
      " |-- Datetime: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Season: integer (nullable = true)\n",
      " |-- Month_of_Year: integer (nullable = true)\n",
      " |-- Day_of_Month: integer (nullable = true)\n",
      " |-- Day_of_Week: integer (nullable = true)\n",
      " |-- Hour_of_Day: double (nullable = true)\n",
      " |-- Number_of_Vehicles: integer (nullable = true)\n",
      " |-- Age_of_Driver: string (nullable = true)\n",
      " |-- Age_of_Vehicle: integer (nullable = true)\n",
      " |-- Junction_Detail: string (nullable = true)\n",
      " |-- Junction_Location: string (nullable = true)\n",
      " |-- X1st_Point_of_Impact: string (nullable = true)\n",
      " |-- Driver_Journey_Purpose: string (nullable = true)\n",
      " |-- Engine_CC: integer (nullable = true)\n",
      " |-- Propulsion_Code: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Vehicle_Category: string (nullable = true)\n",
      " |-- Vehicle_Manoeuvre: string (nullable = true)\n",
      " |-- Accident_Severity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "from pyspark.sql import functions as F\n",
    "accidentesAgeDF = accidentesDF.withColumn(\"Age_of_Driver\", F.when(((F.col(\"Age_of_Driver\") == 1) | (F.col(\"Age_of_Driver\") == 2)), \"Adolescente\")\\\n",
    "                                                            .when(((F.col(\"Age_of_Driver\") == 3) | (F.col(\"Age_of_Driver\") == 4)), \"Joven\")\\\n",
    "                                                            .when(((F.col(\"Age_of_Driver\") == 5) | (F.col(\"Age_of_Driver\") == 6)), \"Adulto\")\\\n",
    "                                                            .otherwise(F.col(\"Age_of_Driver\")))\n",
    "\n",
    "\n",
    "\n",
    "accidentesAgeDF.printSchema() # the column is still in the same position but has now string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44242b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Age_of_Driver='8', count=9195), Row(Age_of_Driver='7', count=13338), Row(Age_of_Driver='Adolescente', count=57174), Row(Age_of_Driver='Adulto', count=67138), Row(Age_of_Driver='Joven', count=104987)]\n"
     ]
    }
   ],
   "source": [
    "assert(dict(accidentesAgeDF.dtypes)[\"Age_of_Driver\"] == \"string\")\n",
    "collectedDF = accidentesAgeDF.groupBy(\"Age_of_Driver\").count().orderBy(\"count\").collect()\n",
    "print(collectedDF)\n",
    "assert((collectedDF[0][\"count\"] == 9195) & (collectedDF[0][\"Age_of_Driver\"] == \"8\"))\n",
    "assert((collectedDF[1][\"count\"] == 13338) & (collectedDF[1][\"Age_of_Driver\"] == \"7\"))\n",
    "assert((collectedDF[2][\"count\"] == 57174) & (collectedDF[2][\"Age_of_Driver\"] == \"Adolescente\"))\n",
    "assert((collectedDF[3][\"count\"] == 67138) & (collectedDF[3][\"Age_of_Driver\"] == \"Adulto\"))\n",
    "assert((collectedDF[4][\"count\"] == 104987) & (collectedDF[4][\"Age_of_Driver\"] == \"Joven\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939ae2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 4:Partiendo de accidentesDF, crear un nuevo DataFrame de una sola fila que contenga, por este orden de columnas, el número de categorías existentes para el propósito del viaje, para el tipo de maniobra del vehículo, para las condiciones de la calzada y para la severidad del accidente. Pista: crear las columnas al vuelo con select(). Renombrar cada columna de conteo para que se llame igual que la propia columna que estamos contando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3154b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------------+-------------+\n",
      "|num_pourpose|num_maniobras|num_condicion_calzada|num_severidad|\n",
      "+------------+-------------+---------------------+-------------+\n",
      "|           5|           11|                    5|            2|\n",
      "+------------+-------------+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import col\n",
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "numeroCategoriasDF = (accidentesDF.select(\"Driver_Journey_Purpose\", \"Vehicle_Manoeuvre\", \"Road_Surface_Conditions\", \"Accident_Severity\")\\\n",
    "                .agg(countDistinct(col(\"Driver_Journey_Purpose\")).alias(\"num_pourpose\"),\\\n",
    "     countDistinct(col(\"Vehicle_Manoeuvre\")).alias(\"num_maniobras\"),\\\n",
    "     countDistinct(col(\"Road_Surface_Conditions\")).alias(\"num_condicion_calzada\"),\\\n",
    "     countDistinct(col(\"Accident_Severity\")).alias(\"num_severidad\")))\n",
    "numeroCategoriasDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d32ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(numeroCategoriasDF.columns) == 4)\n",
    "assert(numeroCategoriasDF.count() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382fe50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 5: Partiendo de accidentesAgeDF definido anteriormente, crear un nuevo DataFrame con tantas filas como posibles \n",
    "#propósitos de un viaje, y tantas columnas como rangos de edad habíamos distinguido en dicho DataFrame más una \n",
    "#(la del propósito del viaje). Las columnas deben llamarse igual que cada uno de los niveles posibles de rangos de edad. \n",
    "#Cada casilla del nuevo DataFrame deberá contener el porcentaje del número de accidentes ocurridos en ese tipo de viaje (fila) \n",
    "#para ese rango de edad (columna), medido sobre el total de accidentes ocurridos para ese tipo de viaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5dbab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|sum(8)|\n",
      "+------+\n",
      "|  9195|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import count\n",
    "viajesPorEdadDF = (accidentesAgeDF.groupBy(\"Driver_Journey_Purpose\")\\\n",
    "                   .pivot(\"Age_of_Driver\")\\\n",
    "                   .count()\\\n",
    "                   .withColumn(\"Total\", F.col(\"Adolescente\")+F.col(\"Adulto\")+F.col(\"Joven\")+F.col(\"7\")+F.col(\"8\"))\\\n",
    "                   .withColumn(\"Adolescente\", F.col(\"Adolescente\") /F.col(\"Total\"))\\\n",
    "                   .withColumn(\"Joven\", (F.col(\"Joven\") / F.col(\"Total\")))\\\n",
    "                   .withColumn(\"Adulto\",( F.col(\"Adulto\")/F.col(\"Total\")))\\\n",
    "                   .withColumn(\"7\", (F.col(\"7\") / F.col(\"Total\")))\\\n",
    "                   .withColumn(\"8\", (F.col(\"8\") / F.col(\"Total\"))))\\\n",
    "                   .drop(\"Total\")\n",
    "\n",
    "accidentesAgeDF.groupBy(\"Driver_Journey_Purpose\")\\\n",
    "                   .pivot(\"Age_of_Driver\")\\\n",
    "                   .count()\\\n",
    ".select(F.sum(col(\"8\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d577cd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Driver_Journey_Purpose: string (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- Adolescente: double (nullable = true)\n",
      " |-- Adulto: double (nullable = true)\n",
      " |-- Joven: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "viajesPorEdadDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce7ae606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Driver_Journey_Purpose='Commuting to/from work', 7=0.012527948326649396, 8=0.0025197856407708414, Adolescente=0.236327501153423, Adulto=0.2791993469851297, Joven=0.46942541789402703)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viajesPorEdadDF.orderBy(\"Driver_Journey_Purpose\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0164e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(viajesPorEdadDF.columns) >= 6)\n",
    "assert(\"7\" in viajesPorEdadDF.columns)\n",
    "assert(\"8\" in viajesPorEdadDF.columns)\n",
    "assert(\"Adolescente\" in viajesPorEdadDF.columns)\n",
    "assert(\"Joven\" in viajesPorEdadDF.columns)\n",
    "assert(\"Adulto\" in viajesPorEdadDF.columns)\n",
    "assert(viajesPorEdadDF.columns[0] == \"Driver_Journey_Purpose\")\n",
    "assert(viajesPorEdadDF.count() == 5)\n",
    "commuting = viajesPorEdadDF.orderBy(\"Driver_Journey_Purpose\").collect()[0]\n",
    "assert(commuting.Driver_Journey_Purpose.startswith(\"Commuting\"))\n",
    "assert(abs(commuting['7'] - 0.012527948326649396) < 0.001)\n",
    "assert(abs(commuting['8'] - 0.002519785640770) < 0.001)\n",
    "assert(abs(commuting.Adolescente - 0.236327501153423) < 0.001)\n",
    "assert(abs(commuting.Adulto - 0.2791993469851297) < 0.001)\n",
    "assert(abs(commuting.Joven - 0.46942541789402703) < 0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "676af3bb",
   "metadata": {},
   "source": [
    "#EJERCICIO 6: Unir la información obtenida en el paso anterior al DataFrame accidentesAgeDF, de manera que al resultado final se añada una columna nueva llamada Porcentaje que contenga el porcentaje de accidentes que ha habido para ese rango de edad y ese tipo de viaje de entre todos los viajes que ha habido de ese tipo (es decir, el porcentaje adecuado de la tabla anterior). Por ejemplo, si el accidente se produjo en un trayecto de tipo Commuting... y la persona es Joven, entonces la columna Porcentaje tomará el valor de la columna Joven y por tanto tendrá el valor 0.46942, pero si la persona es Adulto, entonces tomará el valor de la columna Adulto el cual será 0.2791993469851297."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c647909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
