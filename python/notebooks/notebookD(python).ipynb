{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f7e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"PythonMnMCount\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76e03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejercicio1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc1a0b",
   "metadata": {},
   "source": [
    "Leerlo sin intentar que Spark infiera el tipo de dato de cada columna\n",
    "Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente si al leer le indicamos las siguientes opciones adicionales además de las que ya sueles usar: .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").\n",
    "Asegúrate de que las filas que no tienen el formato correcto sean descartadas, indicando también la opción mode con el valor DROPMALFORMED como vimos en clase.\n",
    "Crear un nuevo DF kivaRawNoNullDF en el que se hayan eliminado todas las filas que tengan algún valor nulo en cualquier columna excepto en la columna tags, que no será relevante para el análisis y por tanto podemos ignorar sus valores nulos y mantener dichas filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e91c1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- funded_amount: string (nullable = true)\n",
      " |-- loan_amount: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- use: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- partner_id: string (nullable = true)\n",
      " |-- posted_time: string (nullable = true)\n",
      " |-- disbursed_time: string (nullable = true)\n",
      " |-- funded_time: string (nullable = true)\n",
      " |-- term_in_months: string (nullable = true)\n",
      " |-- lender_count: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- borrower_genders: string (nullable = true)\n",
      " |-- repayment_interval: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n",
    "from pyspark.sql import functions as F\n",
    "kivaRawDF = (spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"false\")\\\n",
    "        .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "        .load('C:/Users/laura.serrano/Desktop/kiva_loans.csv')).cache() \n",
    "kivaRawDF.printSchema()\n",
    "\n",
    "# Descomentar estas líneas para calcular la lista de columnas que sí deben tenerse en cuenta para quitar nulos. Después\n",
    "# tendrás que utilizar dicha lista en la operación que elimina los nulos\n",
    "columnasExceptoTags = kivaRawDF.columns\n",
    "columnasExceptoTags.remove(\"tags\")\n",
    "kivaRawNoNullDF = kivaRawDF.na.drop(subset=columnasExceptoTags)\n",
    "# Elimina las filas que tenga nulos en todas las columnas excepto en tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc0c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "assert(kivaRawNoNullDF.count() == 574115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d5c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274d549",
   "metadata": {},
   "source": [
    "Las columnas posted_time y disbursed_time son en realidad instantes de tiempo que Spark debería procesar como timestamp. Partiendo de kivaRawNoNullDF, reemplaza ambas columnas por su versión convertida a timestamp, utilizando withColumn con el mismo nombre de cada columna, y donde el nuevo valor de la columna viene dado por el siguiente código:\n",
    "\n",
    "  F.from_unixtime(F.unix_timestamp('nombreColumna', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"))\n",
    "Además, convierte a DoubleType la columna loan_amount y a IntegerType la columna term_in_months.\n",
    "\n",
    "El DF resultante de todas estas operaciones debe quedar almacenado en la variable kivaDF, cacheado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13b3ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "# LÍNEAS EVALUABLES, NO RENOMBRAR LAS VARIABLES\n",
    "kivaDF =(kivaRawNoNullDF.withColumn(\"posted_time\",\\\n",
    "                                  F.from_unixtime(F.unix_timestamp('posted_time', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"))\\\n",
    "                       .withColumn(\"disbursed_time\",\\\n",
    "                                  F.from_unixtime(F.unix_timestamp('disbursed_time', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"))\\\n",
    "                        .withColumn(\"loan_amount\",F.col(\"loan_amount\").cast(DoubleType()))\\\n",
    "                        .withColumn(\"term_in_months\", F.col(\"term_in_months\").cast(IntegerType())))\\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "750f13e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- funded_amount: string (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- use: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- partner_id: string (nullable = true)\n",
      " |-- posted_time: timestamp (nullable = true)\n",
      " |-- disbursed_time: timestamp (nullable = true)\n",
      " |-- funded_time: string (nullable = true)\n",
      " |-- term_in_months: integer (nullable = true)\n",
      " |-- lender_count: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- borrower_genders: string (nullable = true)\n",
      " |-- repayment_interval: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kivaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55bb6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "typesDict = dict(kivaDF.dtypes)\n",
    "assert(typesDict[\"posted_time\"] == \"timestamp\") \n",
    "assert(typesDict[\"disbursed_time\"] == \"timestamp\") \n",
    "assert(typesDict[\"loan_amount\"] == \"double\") \n",
    "assert(typesDict[\"term_in_months\"] == \"int\")\n",
    "cnt_cond = lambda cond: F.sum(F.when(cond, 1).otherwise(0))\n",
    "nullsRow = kivaDF.select(cnt_cond(F.col(\"posted_time\").isNull()).alias(\"posted_nulls\"),\n",
    "              cnt_cond(F.col(\"disbursed_time\").isNull()).alias(\"disbursed_nulls\")).head()\n",
    "assert(nullsRow.posted_nulls == 0)\n",
    "assert(nullsRow.disbursed_nulls == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610d8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29b6ec",
   "metadata": {},
   "source": [
    "Partiendo de kivaDF:\n",
    "\n",
    "Primero, añade una nueva columna dias_desembolso que contenga el número de días que han pasado entre la fecha en que los prestamistas aceptaron financiar un proyecto, y la fecha en que el agente de campo entregó los fondos al beneficiario. Para ello, utiliza withColumn en combinación con la función F.datediff(\"colFuturo\", \"colPasado\")\n",
    "De manera análoga, añade otra nueva columna dias_aceptacion que contenga el número de días entre el anuncio de la necesidad de préstamo y la aceptación de financiarlo por parte de algún prestamista.\n",
    "Reemplazar la columna sector por otra en la que se hayan traducido las categorías \"Education\" por \"Educacion\" (sin tilde para evitar posibles problemas) y \"Agriculture\" por \"Agricultura\", dejando como están el resto de categorías. La sustitución no debe tener más que tres casos: uno para cada categoría que vamos a reemplazar, y un tercero para el resto de categorías, que deben quedarse como estaban.\n",
    "El resultado debe quedar guardado en la variable kivaTiemposDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d27d2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "# imports......\n",
    "kivaTiemposDF = kivaDF.withColumn(\"dias_desembolso\", F.datediff(\"disbursed_time\", F.from_unixtime(F.unix_timestamp('funded_time', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\")))\\\n",
    "                      .withColumn(\"dias_aceptacion\", F.datediff( F.from_unixtime(F.unix_timestamp('funded_time', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"), \"posted_time\"))\\\n",
    "                      .withColumn(\"Sector\", F.when((F.col(\"Sector\"))==\"Education\",\"Educacion\").\\\n",
    "                                  when((F.col(\"sector\") == \"Agriculture\"), \"Agricultura\")\\\n",
    "                                             .otherwise(F.col(\"sector\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "641d09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(kivaTiemposDF.where(\"sector == 'Agricultura'\").count() == 157003)\n",
    "assert(kivaTiemposDF.where(\"sector == 'Educacion'\").count() == 28417)\n",
    "# Comprobamos que las 13 restantes se mantienen sin cambios\n",
    "assert(kivaTiemposDF.groupBy(\"sector\").count().join(kivaDF.groupBy(\"sector\").count(), [\"sector\", \"count\"]).count() == 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d560ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd5212",
   "metadata": {},
   "source": [
    "Partiendo de kivaTiemposDF, crear un nuevo DataFrame llamado kivaAgrupadoDF que tenga:\n",
    "\n",
    "Tantas filas como países (country; no usar el código de país), y tantas columnas como sectores (cada una llamada como el sector) más una (la columna del país, que debe aparecer en primer lugar). En cada celda deberá ir el número medio (redondeado a 2 cifras decimales) de días transcurridos en ese país y sector entre la fecha en que se anuncia la necesidad de préstamo y la fecha en la que un prestamista acepta financiarlo. Esta columna ha sido calculada en la celda precedente.\n",
    "Después de esto, añadir una columna adicional transcurrido_global con el número medio (redondeado a 2 cifras decimales) de días transcurridos en cada país entre ambas fechas sin tener en cuenta el sector. Cada fila tendrá la media de las 15 columnas del apartado anterior.\n",
    "Por último, ordenar el DF resultante descendentemente en base al tiempo medio global, transcurrido_global. El DF resultado de la ordenación debe ser almacenado en la variable kivaAgrupadoDF.\n",
    "PISTA: utiliza el método pivot con el sector para el primer apartado, envolviendo a la función de agregación con la función F.round, es decir, F.round(F.funcionAgregacion(....), 2), y withColumn con una operación aritmética entre columnas en el segundo. No debe utilizarse la función when ya que Spark es capaz de hacer directamente aritmética entre objetos columna. La operación aritmética también debe estar envuelta por round: F.round(op. aritmética entre objetos columna, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1630112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "kivaAgrupadoDF1 = (kivaTiemposDF.groupBy(\"country\").pivot(\"sector\")\\\n",
    "                              .agg(F.round(F.avg(F.col(\"dias_aceptacion\")), 2)))\n",
    "kivaAgrupadoDF=kivaAgrupadoDF1.withColumn(\"transcurrido_global\",F.round(reduce(add,\\\n",
    "                                    [F.col(x) for x in kivaAgrupadoDF1.columns[1:]])/(len(kivaAgrupadoDF1.columns) - 1), 2))\\\n",
    ".orderBy(\"transcurrido_global\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6eb3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = kivaAgrupadoDF.head()\n",
    "assert(r1.country == \"United States\")\n",
    "assert((r1.Agricultura - 12.0 < 0.01) | (r1.Agricultura - 12.17 < 0.01))\n",
    "assert((r1.Educacion - 15.21 < 0.01) | (r1.Educacion - 15.33 < 0.01))\n",
    "assert(r1.Wholesale - 27.5 < 0.01)\n",
    "assert((r1.transcurrido_global - 20.94 < 0.01) | (r1.transcurrido_global - 21.04 < 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce60afc",
   "metadata": {},
   "source": [
    "Partiendo de nuevo de kivaTiemposDF, añadir las siguientes columnas:\n",
    "\n",
    "Primero, tres columnas adicionales llamadas transc_medio, transc_min, transc_max que contengan, respectivamente, el número de días medio, mínimo y máximo transcurrido para proyectos de ese mismo país y ese mismo sector entre la fecha en que se postea la necesidad de préstamo y la fecha en la que alguien acepta financiarlo (es decir, la columna dias_aceptacion calculada antes y utilizada también en la celda anterior). Es decir, queremos una columna extra para que podamos tener, junto a cada préstamo, información agregada de los préstamos similares, entendidos como aquellos del mismo país y del mismo sector. No se debe utilizar JOIN sino solo funciones de ventana.\n",
    "Finalmente, crear otra columna adicional diff_dias que contenga la diferencia en días entre los días que transcurrieron en este proyecto y la media de días de los proyectos similares (calculada en el apartado anterior). Debería ser lo primero menos lo segundo, de forma que un número positivo indica que este préstamo tardó más días en ser aceptado que la media de días de este país y sector, y un número negativo indica lo contrario. El resultado debe obtenerse aplicando operaciones aritméticas con columnas existentes, sin utilizar when.\n",
    "El DF resultante con las 4 columnas nuevas que hemos añadido debe quedar almacenado en la variable kivaExtraInfoDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "350093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "from pyspark.sql import Window\n",
    "windowPaisSector = Window().partitionBy(\"country\", \"sector\")\n",
    "kivaExtraInfoDF = kivaTiemposDF.withColumn(\"transc_medio\", F.mean(\"dias_aceptacion\").over(windowPaisSector))\\\n",
    "                               .withColumn(\"transc_min\", F.min(\"dias_aceptacion\").over(windowPaisSector))\\\n",
    "                               .withColumn(\"transc_max\", F.max(\"dias_aceptacion\").over(windowPaisSector))\\\n",
    "                               .withColumn(\"diff_dias\",  ((F.datediff(\"funded_time\", \"posted_time\")) - F.col(\"dias_aceptacion\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3ca3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = kivaExtraInfoDF.where(\"id = '658540'\").head()\n",
    "assert(r.country == 'Burkina Faso')\n",
    "assert(r.transc_medio - 11.02 < 0.05)\n",
    "assert(r.dias_aceptacion == 35)\n",
    "assert(r.diff_dias - 24.0 < 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da74ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
